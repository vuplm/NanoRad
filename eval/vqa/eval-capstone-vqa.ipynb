{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13725249,"sourceType":"datasetVersion","datasetId":8732440},{"sourceId":13837855,"sourceType":"datasetVersion","datasetId":8813184},{"sourceId":13910929,"sourceType":"datasetVersion","datasetId":8863636},{"sourceId":13910939,"sourceType":"datasetVersion","datasetId":8863644},{"sourceId":14012740,"sourceType":"datasetVersion","datasetId":8918686}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning\n!pip install -U transformers","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-12-12T02:21:01.036013Z","iopub.execute_input":"2025-12-12T02:21:01.036214Z","iopub.status.idle":"2025-12-12T02:23:00.529476Z","shell.execute_reply.started":"2025-12-12T02:21:01.036190Z","shell.execute_reply":"2025-12-12T02:23:00.528768Z"}},"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.6.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.3)\nRequirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.10.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.15.2)\nRequirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (25.0)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.6.0+cu124)\nRequirement already satisfied: torchmetrics<3.0,>0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.8.2)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\nRequirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.15.0)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.5)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.13.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.20.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics<3.0,>0.7.0->lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.22.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.3)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.11)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>0.7.0->lightning) (2024.2.0)\nDownloading lightning-2.6.0-py3-none-any.whl (845 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lightning\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed lightning-2.6.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m935.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\nSuccessfully installed tokenizers-0.22.1 transformers-4.57.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\nsnapshot_download(\n    repo_id=\"huyhoangt2201/lightmedvlm-mimic-phase3-vqa-reduced\",\n    local_dir=\"lightmedvlm\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T02:23:05.004675Z","iopub.execute_input":"2025-12-12T02:23:05.005407Z","iopub.status.idle":"2025-12-12T02:23:59.785071Z","shell.execute_reply.started":"2025-12-12T02:23:05.005373Z","shell.execute_reply":"2025-12-12T02:23:59.784456Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ea59abaf72242189b50afd7c360c432"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"logs/tensorboard/version_0/events.out.tf(…):   0%|          | 0.00/12.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f1c16691cf493da2627cdbc899ff0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"checkpoints/epoch=3-step=2468-loss=0.078(…):   0%|          | 0.00/2.31G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee49a922bee54c88b1776782c4202e0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"val_pred.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccd140618e26416799880f372f47cc0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"val_ref.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e88262f66f94a5896a938610d908ffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hparams.yaml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3b59ed19af4b209068d370ae1ee731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5baa7a578e5d489b93d7fd210d96d14f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"metrics.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db301ec16ac84e8e92dcd5378948dbeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hparams.yaml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"523200f8a95e4ca08e75f59b3e79a0d9"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/lightmedvlm'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport lightning.pytorch as pl\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import SwinModel\nfrom peft import get_peft_model, LoraConfig, TaskType\nimport torch.distributed as dist\nfrom transformers import BertTokenizer, AutoImageProcessor\nfrom PIL import Image\nimport numpy as np\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, inter_dim, out_dim):\n        super(MLP, self).__init__()\n        self.hidden_1 = nn.Linear(in_dim, inter_dim)\n        self.act = nn.GELU()\n        self.hidden_2 = nn.Linear(inter_dim, out_dim)\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, x):\n        x = self.act(self.hidden_1(x))\n        x = self.dropout(x)\n        return self.hidden_2(x)\n\n\nclass LightMedVLMForInference(pl.LightningModule):\n    def __init__(\n        self,\n        vision_model: str = \"microsoft/swin-base-patch4-window7-224\",\n        llm_model: str = \"Qwen/Qwen3-0.6B\",\n\n        # For training setup\n        vis_use_lora: bool = False,\n        vis_r: int = 8,\n        vis_alpha: int = 16,\n        freeze_vm: bool = False,\n        llm_use_lora: bool = False,\n        llm_r: int = 8,\n        llm_alpha: int = 16,\n        lora_dropout: float = 0.1,\n        low_resource: bool = False,\n        max_length: int = 256\n    ):\n        super().__init__()\n        self.vision_model = vision_model\n        self.llm_model = llm_model\n        self.vis_use_lora = vis_use_lora\n        self.vis_r = vis_r\n        self.vis_alpha = vis_alpha\n        self.freeze_vm = freeze_vm\n        self.llm_use_lora = llm_use_lora\n        self.llm_r = llm_r\n        self.llm_alpha = llm_alpha\n        self.lora_dropout = lora_dropout\n        self.low_resource = low_resource\n        self.max_length = max_length\n        \n        # Vision encoder setup\n        print(f'Loading vision encoder: {self.vision_model}')\n        self.visual_encoder = SwinModel.from_pretrained(self.vision_model)\n        self.vit_feature_extractor = AutoImageProcessor.from_pretrained(self.vision_model)\n        if self.vis_use_lora:\n            peft_config_visual = LoraConfig(\n                r=self.vis_r,\n                lora_alpha=self.vis_alpha,\n                target_modules=[\"query\", \"value\"],\n                lora_dropout=self.lora_dropout,\n                bias=\"none\",\n                modules_to_save=[\"classifier\"],\n            )\n            self.visual_encoder = get_peft_model(self.visual_encoder, peft_config_visual)\n            self.visual_encoder.print_trainable_parameters()\n            print('Loading vision encoder with LoRA -- Done')\n        elif self.freeze_vm:\n            for name, param in self.visual_encoder.named_parameters():\n                param.requires_grad = False\n            print(f'Loading Frozen vision encoder: {self.vision_model} -- Done')\n        else:\n            print(f'Loading Trainable vision encoder: {self.vision_model} -- Done')\n\n        # LLM model setup\n        print('Loading LLM model')\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.llm_model, \n            use_fast=False,\n            trust_remote_code=True\n        )\n        print(f\"BOS token ID: {self.tokenizer.bos_token_id}\")\n        print(f\"EOS token ID: {self.tokenizer.eos_token_id}\")\n        print(f\"PAD token ID: {self.tokenizer.pad_token_id}\")\n        if self.low_resource:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.llm_model,\n                torch_dtype=torch.bfloat16,\n                load_in_8bit=True,\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n        else:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.llm_model,\n                torch_dtype=torch.bfloat16,\n                trust_remote_code=True\n            )\n        if self.llm_use_lora:\n            self.embed_tokens = self.model.get_input_embeddings()\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM, \n                inference_mode=False, \n                r=self.llm_r, \n                lora_alpha=self.llm_alpha, \n                lora_dropout=self.lora_dropout,\n                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  \n            )\n            self.model = get_peft_model(self.model, peft_config)\n            self.model.print_trainable_parameters()\n            print('Loading LLM LoRA Done')         \n        else:\n            self.embed_tokens = self.model.get_input_embeddings()\n            for name, param in self.model.named_parameters():\n                param.requires_grad = False\n            print('Loading LLM Done')\n\n        # Projector setup\n        self.proj = MLP(\n            in_dim=self.visual_encoder.num_features,\n            inter_dim=2048,\n            out_dim=self.model.config.hidden_size\n        )\n        self.layer_norm = nn.LayerNorm(self.model.config.hidden_size)\n\n        # System prompt setup\n        self.end_sym = \"<|im_end|>\"\n        # System prompt for VQA task\n        self.system_prompt = \"<|im_start|>system\\nYou are a professional radiologist. Please answer the question based on the chest X-ray image and choose from the following two options: [yes, no].<|im_end|>\\n\"\n        \n        self.val_step_outputs = []\n        self.test_step_outputs = []\n        self.val_score = 0.0\n        \n    def encode_img(self, images):\n        image_embeds = []\n        for image in images:\n            device = image.device\n            # Swin transformer\n            visual_outputs = self.visual_encoder(image)\n            image_embed = visual_outputs['last_hidden_state'].to(device)\n            image_embeds.append(image_embed)\n            \n        image_embeds = torch.stack(image_embeds).mean(0)\n\n        inputs = self.proj(image_embeds)\n        atts = torch.ones(inputs.size()[:-1], dtype=torch.long).to(image.device)\n        return inputs, atts\n\n    def prompt_wrap(self, img_embeds, atts_img, questions):\n        \"\"\"\n        Wrap image embeddings with Qwen-style prompt including the question.\n        Format: {system_prompt} <user_start> {question} <image> <user_end> <assistant_start>\n        \n        Args:\n            img_embeds: Image embeddings\n            atts_img: Attention mask for images\n            questions: List of questions (one per batch item)\n        \"\"\"\n        batch_size = img_embeds.shape[0]\n        \n        # Build prompts for each item in the batch\n        wrapped_embeds_list = []\n        wrapped_atts_list = []\n        \n        for i in range(batch_size):\n            question = questions[i] if questions[i] is not None else \"Describe the following image in detail.\"\n            \n            # Construct full prompt with question\n            full_prompt = f\"{self.system_prompt}<|im_start|>user\\n{question} <image><|im_end|>\\n<|im_start|>assistant\\n\"\n            \n            # Split at image placeholder\n            p_before, p_after = full_prompt.split('<image>')\n            \n            # Tokenize prompt parts\n            p_before_tokens = self.tokenizer(\n                p_before,\n                return_tensors=\"pt\",\n                add_special_tokens=False\n            ).to(img_embeds.device)\n            \n            p_after_tokens = self.tokenizer(\n                p_after,\n                return_tensors=\"pt\",\n                add_special_tokens=False\n            ).to(img_embeds.device)\n            \n            # Get embeddings\n            with torch.no_grad():\n                p_before_embeds = self.embed_tokens(p_before_tokens.input_ids)\n                p_after_embeds = self.embed_tokens(p_after_tokens.input_ids)\n            \n            # Concatenate: [prompt_before] + [image] + [prompt_after]\n            wrapped_embeds = torch.cat([\n                p_before_embeds,\n                img_embeds[i:i+1],\n                p_after_embeds\n            ], dim=1)\n            \n            wrapped_embeds_list.append(wrapped_embeds)\n            \n            # Create attention mask\n            wrapped_atts = torch.ones(\n                wrapped_embeds.shape[1],\n                device=img_embeds.device,\n                dtype=atts_img.dtype\n            )\n            wrapped_atts_list.append(wrapped_atts)\n        \n        # Find max sequence length in the batch\n        max_seq_len = max(embeds.shape[1] for embeds in wrapped_embeds_list)\n        \n        # Pad all sequences to the same length\n        padded_embeds_list = []\n        padded_atts_list = []\n        \n        for embeds, atts in zip(wrapped_embeds_list, wrapped_atts_list):\n            seq_len = embeds.shape[1]\n            if seq_len < max_seq_len:\n                # Pad embeddings with zeros\n                padding_size = max_seq_len - seq_len\n                padding = torch.zeros(\n                    embeds.shape[0], \n                    padding_size, \n                    embeds.shape[2],\n                    dtype=embeds.dtype,\n                    device=embeds.device\n                )\n                embeds = torch.cat([embeds, padding], dim=1)\n                \n                # Pad attention mask with zeros (masked positions)\n                atts_padding = torch.zeros(\n                    padding_size,\n                    dtype=atts.dtype,\n                    device=atts.device\n                )\n                atts = torch.cat([atts, atts_padding], dim=0)\n            \n            padded_embeds_list.append(embeds)\n            padded_atts_list.append(atts)\n        \n        # Stack all items in the batch\n        wrapped_img_embeds = torch.cat(padded_embeds_list, dim=0)\n        wrapped_atts_img = torch.stack(padded_atts_list, dim=0)\n        \n        return wrapped_img_embeds, wrapped_atts_img\n\n    def forward(self, samples):\n        image = samples[\"image\"]\n        questions = samples.get(\"question\", [None] * len(samples[\"text\"]))\n        \n        img_embeds, atts_img = self.encode_img(image)\n        img_embeds = self.layer_norm(img_embeds)\n\n        # Wrap image with prompt (now includes question)\n        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, questions)\n\n        self.tokenizer.padding_side = \"right\"\n        text = [t + self.end_sym for t in samples[\"text\"]]\n\n        # Tokenize target text\n        to_regress_tokens = self.tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.hparams.max_length,\n            add_special_tokens=False\n        ).to(image[0].device)\n\n        # Create labels: mask prompt+image tokens with -100, keep text tokens\n        targets = to_regress_tokens.input_ids.masked_fill(\n            to_regress_tokens.input_ids == self.tokenizer.pad_token_id, -100\n        )\n\n        # Create empty targets for prompt+image tokens\n        empty_targets = (\n            torch.ones([atts_img.shape[0], atts_img.shape[1]],\n                       dtype=torch.long).to(image[0].device).fill_(-100)\n        )\n        targets = torch.cat([empty_targets, targets], dim=1)\n\n        # Get text embeddings\n        with torch.no_grad():\n            to_regress_embeds = self.embed_tokens(to_regress_tokens.input_ids)\n        \n        # Concatenate all embeddings: [prompt+image] + [text]\n        inputs_embeds = torch.cat([img_embeds, to_regress_embeds], dim=1)\n        attention_mask = torch.cat([atts_img, to_regress_tokens.attention_mask], dim=1)\n\n        # Forward through LLM\n        outputs = self.model(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            return_dict=True,\n            labels=targets,\n            use_cache=False\n        )\n        all_loss = outputs.loss\n\n        return {\"loss\": all_loss}\n\n    def training_step(self, batch, batch_idx):\n        result = self(batch)\n        self.log_dict(result, prog_bar=True)\n        return result\n\n    def save_checkpoint(self, eval_res):\n        current_epoch, global_step = self.trainer.current_epoch, self.trainer.global_step\n        param_grad_dic = {\n            k: v.requires_grad for (k, v) in self.named_parameters() if v.requires_grad\n        }\n        state_dict = self.state_dict()\n        for k in list(state_dict.keys()):\n            if k not in param_grad_dic.keys():\n                del state_dict[k]\n        \n        save_obj = {\n            \"state_dict\": state_dict,\n            \"hyper_parameters\": self.hparams,\n            \"pytorch-lightning_version\": pl.__version__,\n            \"epoch\": current_epoch,\n            \"global_step\": global_step,\n        }\n        os.makedirs(os.path.join(self.hparams.savedmodel_path, 'checkpoints'), exist_ok=True)\n        save_to = os.path.join(\n            self.hparams.savedmodel_path, 'checkpoints',\n            \"checkpoint_epoch{}_step{}_rougle_l{:3}_bleu{:3f}_cider{:3f}.pth\".format(\n                current_epoch, global_step, eval_res['ROUGE_L'],eval_res['Bleu_4'], eval_res['CIDEr']\n            ),\n        )\n        self.print(\"Saving checkpoint at step {} to {}.\".format(global_step, save_to))\n        torch.save(save_obj, save_to)\n    \n    def decode(self, output_token):\n        \"\"\"Decode output tokens to text.\"\"\"\n        # Remove special tokens at the beginning\n        if len(output_token) > 0 and output_token[0] == self.tokenizer.pad_token_id:\n            output_token = output_token[1:]\n        if len(output_token) > 0 and output_token[0] == self.tokenizer.bos_token_id:\n            output_token = output_token[1:]\n        \n        # Decode to text\n        output_text = self.tokenizer.decode(output_token, add_special_tokens=False)\n        \n        # Split at end symbol and clean up\n        output_text = output_text.split(self.end_sym)[0].strip()\n        \n        # Remove Qwen special tokens\n        output_text = output_text.replace('<|im_start|>', '')\n        output_text = output_text.replace('<|im_end|>', '')\n        output_text = output_text.replace('<|endoftext|>', '')\n        output_text = output_text.replace('<unk>', '')\n        \n        return output_text\n\n    def _parse_image(self, img):\n        pixel_values = self.vit_feature_extractor(img, return_tensors=\"pt\").pixel_values\n        return pixel_values[0] \n        \n    @torch.no_grad()\n    def inference(self, image_paths, question=None, beam_size=1, do_sample=False, \n                 min_new_tokens=1, max_new_tokens=100, repetition_penalty=1.0, \n                 length_penalty=1.0, temperature=1.0):\n        \"\"\"\n        Generate answer from images and question.\n        \n        Args:\n            image_paths: List of image paths\n            question: Question text (optional, defaults to general description)\n            beam_size, do_sample, etc.: Generation parameters\n        \"\"\"\n        self.eval()\n\n        images = []\n        device = next(self.parameters()).device\n        for image_path in image_paths:\n            with Image.open(image_path) as pil:\n                array = np.array(pil, dtype=np.uint8)\n                if array.shape[-1] != 3 or len(array.shape) != 3:\n                    array = np.array(pil.convert(\"RGB\"), dtype=np.uint8)\n                image = self._parse_image(array)\n                image = image.unsqueeze(0).to(device)\n                images.append(image)\n\n        dtype = self.model.dtype\n        \n        img_embeds, atts_img = self.encode_img(images)\n        img_embeds = self.layer_norm(img_embeds)\n\n        img_embeds = img_embeds.to(dtype)\n        \n        # Use the question in the prompt\n        if question is None:\n            question = \"Describe the following image in detail.\"\n        questions = [question]\n        \n        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, questions)\n\n        outputs = self.model.generate(\n            inputs_embeds=img_embeds,\n            attention_mask=atts_img,\n            num_beams=beam_size,\n            do_sample=do_sample,\n            min_new_tokens=min_new_tokens,\n            max_new_tokens=max_new_tokens,\n            repetition_penalty=repetition_penalty,\n            length_penalty=length_penalty,\n            temperature=temperature,\n            pad_token_id=self.tokenizer.pad_token_id,\n            eos_token_id=self.tokenizer.eos_token_id,\n        )\n        \n        hypo = [self.decode(i) for i in outputs]\n        return hypo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T02:24:05.804409Z","iopub.execute_input":"2025-12-12T02:24:05.805182Z","iopub.status.idle":"2025-12-12T02:24:56.996390Z","shell.execute_reply.started":"2025-12-12T02:24:05.805153Z","shell.execute_reply":"2025-12-12T02:24:56.995805Z"}},"outputs":[{"name":"stderr","text":"2025-12-12 02:24:25.591792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765506266.001422      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765506266.119632      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"from lightning.fabric.utilities.data import AttributeDict\ntorch.serialization.add_safe_globals([AttributeDict])\n\nckpt_file=\"lightmedvlm/checkpoints/epoch=3-step=2468-loss=0.0781.ckpt\"   # Absoluate path to .pth file\nargs = {\n    \"vision_model\":\"microsoft/swin-base-patch4-window7-224\",\n    \"llm_model\":\"Qwen/Qwen3-0.6B\"\n}\nmodel = LightMedVLMForInference.load_from_checkpoint(ckpt_file,strict=False, **args)\nmodel = model.to(\"cuda\")\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T02:24:56.997350Z","iopub.execute_input":"2025-12-12T02:24:56.997923Z","iopub.status.idle":"2025-12-12T02:25:09.199166Z","shell.execute_reply.started":"2025-12-12T02:24:56.997903Z","shell.execute_reply":"2025-12-12T02:25:09.198479Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Loading vision encoder: microsoft/swin-base-patch4-window7-224\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d594f25cedf4138abb2409b105bbb7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc2c2a347f2e438b8725cfaf521792ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a08a9da7afdc41c589f232b899393c41"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"Loading Trainable vision encoder: microsoft/swin-base-patch4-window7-224 -- Done\nLoading LLM model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0731491258412cb78d2fc61a32125c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c221201e17b4f8a8f2b17b8a6fde2f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4632733c07f468dbdf862a77aa29f28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e66f2b5be543fd9c5a1cf99622b33f"}},"metadata":{}},{"name":"stdout","text":"BOS token ID: None\nEOS token ID: 151645\nPAD token ID: 151643\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34aa58f5b0d14a4b9502c8f7ba146a91"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaf3ef173bf84c0cb5f2e872ce0d6459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af10cbff5aa428aa0f111e448d159b1"}},"metadata":{}},{"name":"stdout","text":"trainable params: 2,293,760 || all params: 598,343,680 || trainable%: 0.3834\nLoading LLM LoRA Done\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"LightMedVLMForInference(\n  (visual_encoder): SwinModel(\n    (embeddings): SwinEmbeddings(\n      (patch_embeddings): SwinPatchEmbeddings(\n        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n      )\n      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): SwinEncoder(\n      (layers): ModuleList(\n        (0): SwinStage(\n          (blocks): ModuleList(\n            (0): SwinLayer(\n              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=128, out_features=128, bias=True)\n                  (key): Linear(in_features=128, out_features=128, bias=True)\n                  (value): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): Identity()\n              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (1): SwinLayer(\n              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=128, out_features=128, bias=True)\n                  (key): Linear(in_features=128, out_features=128, bias=True)\n                  (value): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.004347826354205608)\n              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=512, out_features=256, bias=False)\n            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (1): SwinStage(\n          (blocks): ModuleList(\n            (0): SwinLayer(\n              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=256, out_features=256, bias=True)\n                  (key): Linear(in_features=256, out_features=256, bias=True)\n                  (value): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.008695652708411217)\n              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=256, out_features=1024, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=1024, out_features=256, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (1): SwinLayer(\n              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=256, out_features=256, bias=True)\n                  (key): Linear(in_features=256, out_features=256, bias=True)\n                  (value): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.013043479062616825)\n              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=256, out_features=1024, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=1024, out_features=256, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (2): SwinStage(\n          (blocks): ModuleList(\n            (0): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.017391305416822433)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (1): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.021739132702350616)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (2): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.02608695812523365)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (3): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.030434783548116684)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (4): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.03478261083364487)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (5): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.03913043811917305)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (6): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.04347826540470123)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (7): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.04782608896493912)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (8): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.052173912525177)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (9): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.056521736085414886)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (10): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.06086956337094307)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (11): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.06521739065647125)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (12): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.06956521421670914)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (13): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.07391304522752762)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (14): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.0782608687877655)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (15): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.08260869979858398)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (16): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.08695652335882187)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (17): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.09130434691905975)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (3): SwinStage(\n          (blocks): ModuleList(\n            (0): SwinLayer(\n              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.09565217792987823)\n              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (1): SwinLayer(\n              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.10000000149011612)\n              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n    )\n    (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (pooler): AdaptiveAvgPool1d(output_size=1)\n  )\n  (model): PeftModelForCausalLM(\n    (base_model): LoraModel(\n      (model): Qwen3ForCausalLM(\n        (model): Qwen3Model(\n          (embed_tokens): Embedding(151936, 1024)\n          (layers): ModuleList(\n            (0-27): 28 x Qwen3DecoderLayer(\n              (self_attn): Qwen3Attention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n                (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              )\n              (mlp): Qwen3MLP(\n                (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n                (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n                (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n                (act_fn): SiLUActivation()\n              )\n              (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n              (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            )\n          )\n          (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n          (rotary_emb): Qwen3RotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n      )\n    )\n  )\n  (embed_tokens): Embedding(151936, 1024)\n  (proj): MLP(\n    (hidden_1): Linear(in_features=1024, out_features=2048, bias=True)\n    (act): GELU(approximate='none')\n    (hidden_2): Linear(in_features=2048, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model.inference(image_paths=[\"/kaggle/input/mimic-700-images/iu_images/iu_images/CXR3030_IM-1405/0.png\"], question=\"Does the cardiomediastinal silhouette appear normal in the chest X-ray?\")  # Inference one report at a time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T10:22:44.863525Z","iopub.execute_input":"2025-12-09T10:22:44.864024Z","iopub.status.idle":"2025-12-09T10:22:46.169872Z","shell.execute_reply.started":"2025-12-09T10:22:44.864003Z","shell.execute_reply":"2025-12-09T10:22:46.169088Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['Yes, the cardiomediastinal silhouette appears normal in the chest X-ray.']"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Infer IU","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nTEST_ANNOT = \"/kaggle/input/mimic-700-images/test/test/vqa/iuxray_test.jsonl\"\nTEST_IMG_DIR = \"/kaggle/input/mimic-700-images/iu_images/iu_images\"\n\n# Load JSONL\nwith open(TEST_ANNOT, \"r\") as f:\n    lines = f.readlines()\n\nprint(\"Số lượng test samples:\", len(lines))\n\nresults = []\n\nfor line in tqdm(lines):\n    item = json.loads(line)\n\n    img_path = os.path.join(TEST_IMG_DIR, item[\"image\"])\n\n    # --- FIX CLEAN QUESTION ---\n    raw_question = item[\"question\"]\n    question = (\n        raw_question\n        .replace(\"\\n<image>\", \"\")\n        .replace(\"<image>\", \"\")\n        .strip()\n    )\n\n    gt_answer = item[\"answer\"]\n\n    # --- INFER ---\n    pred = model.inference(\n        image_paths=[img_path],\n        question=question\n    )[0]\n\n    # --- SAVE ---\n    results.append({\n        \"id\": item[\"question_id\"],\n        \"image\": item[\"image\"],\n        \"question\": question,\n        \"gt_answer\": gt_answer,\n        \"pred_answer\": pred\n    })\n\nOUT_FILE = \"iuxray_vqa_predictions.json\"\n\nwith open(OUT_FILE, \"w\") as f:\n    json.dump(results, f, indent=4, ensure_ascii=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T13:30:22.873548Z","iopub.execute_input":"2025-12-11T13:30:22.874124Z","iopub.status.idle":"2025-12-11T14:29:32.758622Z","shell.execute_reply.started":"2025-12-11T13:30:22.874096Z","shell.execute_reply":"2025-12-11T14:29:32.757732Z"}},"outputs":[{"name":"stdout","text":"Số lượng test samples: 2573\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/2573 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n100%|██████████| 2573/2573 [59:09<00:00,  1.38s/it]  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Test MIMIC\n","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nTEST_ANNOT = \"/kaggle/input/mimic-700-images/test/test/vqa/mimic_test.jsonl\"\nTEST_IMG_DIR = \"/kaggle/input/mimic-700-images/mimic_cxr_selected_224/mimic_cxr_selected_224\"\n\nwith open(TEST_ANNOT, \"r\") as f:\n    lines = f.readlines()\n\nprint(\"Số lượng test samples:\", len(lines))\n\nresults = []\n\nfor line in tqdm(lines):\n    item = json.loads(line)\n\n    img_path = os.path.join(TEST_IMG_DIR, item[\"image\"])\n\n    raw_question = item[\"question\"]\n    question = (\n        raw_question\n        .replace(\"\\n<image>\", \"\")\n        .replace(\"<image>\", \"\")\n        .strip()\n    )\n\n    gt_answer = item[\"answer\"]\n\n    pred = model.inference(\n        image_paths=[img_path],\n        question=question\n    )[0]\n\n    results.append({\n        \"id\": item[\"question_id\"],\n        \"image\": item[\"image\"],\n        \"question\": question,\n        \"gt_answer\": gt_answer,\n        \"pred_answer\": pred\n    })\n\nOUT_FILE = \"mimic_vqa_predictions.json\"\n\nwith open(OUT_FILE, \"w\") as f:\n    json.dump(results, f, indent=4, ensure_ascii=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T02:25:26.467418Z","iopub.execute_input":"2025-12-12T02:25:26.468129Z","iopub.status.idle":"2025-12-12T03:34:17.763958Z","shell.execute_reply.started":"2025-12-12T02:25:26.468102Z","shell.execute_reply":"2025-12-12T03:34:17.763281Z"}},"outputs":[{"name":"stdout","text":"Số lượng test samples: 3470\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/3470 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n100%|██████████| 3470/3470 [1:08:51<00:00,  1.19s/it]\n","output_type":"stream"}],"execution_count":5}]}