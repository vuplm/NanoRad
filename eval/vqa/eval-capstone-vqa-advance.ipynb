{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14012740,"sourceType":"datasetVersion","datasetId":8918686}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning\n!pip install -U transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\nsnapshot_download(\n    repo_id=\"vuplm2004/lightmedvlm-mimic-phase3-vqa-reduced\",\n    local_dir=\"lightmedvlm\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:11:48.357127Z","iopub.execute_input":"2025-12-13T16:11:48.357425Z","iopub.status.idle":"2025-12-13T16:11:56.848245Z","shell.execute_reply.started":"2025-12-13T16:11:48.357403Z","shell.execute_reply":"2025-12-13T16:11:56.847355Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ebd1ec7db0f4253b8e695609d468b88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"val_ref.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd54e41b6a4e4cc1921ab0fe954f0cc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"val_pred.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fed940bf2b14833878f8d5d5cd83065"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e6a5684b184cc2977254eb548ab40c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"checkpoints/epoch=3-step=2468.ckpt:   0%|          | 0.00/2.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0c8dc4545e426382c29e18320e4df1"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/lightmedvlm'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport lightning.pytorch as pl\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import SwinModel\nfrom peft import get_peft_model, LoraConfig, TaskType\nimport torch.distributed as dist\nfrom transformers import BertTokenizer, AutoImageProcessor\nfrom PIL import Image\nimport numpy as np\nimport math\n\n# ==============================================================================\n# REPLACED MLP WITH HYBRID VISION ABSTRACTOR\n# ==============================================================================\nclass HybridVisionAbstractor(nn.Module):\n    def __init__(self, visual_dim, llm_dim, num_queries=64):\n        super().__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(visual_dim, visual_dim, 3, 1, 1), nn.BatchNorm2d(visual_dim), nn.SiLU(),\n            nn.Conv2d(visual_dim, visual_dim, 3, 1, 1), nn.BatchNorm2d(visual_dim), nn.SiLU()\n        )\n        self.vis_proj = nn.Linear(visual_dim, llm_dim)\n        self.latents = nn.Parameter(torch.randn(1, num_queries, llm_dim))\n        self.cross_attn = nn.MultiheadAttention(llm_dim, 8, batch_first=True, dropout=0.1)\n        self.ln_q, self.ln_v, self.ln_out = nn.LayerNorm(llm_dim), nn.LayerNorm(llm_dim), nn.LayerNorm(llm_dim)\n        self.mlp = nn.Sequential(nn.Linear(llm_dim, llm_dim*4), nn.GELU(), nn.Linear(llm_dim*4, llm_dim))\n\n    def inject_concept_init(self, concept_embeds):\n        with torch.no_grad():\n            # Inject concepts into the first N learnable queries\n            self.latents[:, :concept_embeds.shape[0], :] = concept_embeds.unsqueeze(0)\n        print(f\">> INJECTED {concept_embeds.shape[0]} DISEASE CONCEPTS <<\")\n\n    def forward(self, visual_features):\n        # visual_features: (Batch, Seq_Len, Vis_Dim)\n        B, L, C = visual_features.shape\n        H = W = int(L**0.5)\n        \n        # Spatial Refinement (CNN)\n        x_img = visual_features.permute(0, 2, 1).view(B, C, H, W)\n        x_refined = self.conv_block(x_img).flatten(2).permute(0, 2, 1)\n        \n        # Projection\n        x_vis = self.vis_proj(x_refined)\n        \n        # Cross Attention (Queries attending to Image)\n        q = self.ln_q(self.latents.repeat(B, 1, 1))\n        k = v = self.ln_v(x_vis)\n        attn_out, _ = self.cross_attn(q, k, v)\n        \n        # Residual + MLP\n        x = self.ln_out(q + attn_out)\n        x = x + self.mlp(x)\n        return x\n\nclass LightMedVLMForInfer(pl.LightningModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.save_hyperparameters(args)\n        print(f'Loading vision encoder: {args.vision_model}')\n        self.visual_encoder = SwinModel.from_pretrained(args.vision_model)\n        self.vit_feature_extractor = AutoImageProcessor.from_pretrained(args.vision_model)\n        if args.vis_use_lora:\n            peft_config_visual = LoraConfig(\n                r=args.vis_r,\n                lora_alpha=args.vis_alpha,\n                target_modules=[\"query\", \"value\"],\n                lora_dropout=args.lora_dropout,\n                bias=\"none\",\n                modules_to_save=[\"classifier\"],\n            )\n            self.visual_encoder = get_peft_model(self.visual_encoder, peft_config_visual)\n            self.visual_encoder.print_trainable_parameters()\n            print('Loading vision encoder with LoRA -- Done')\n        elif args.freeze_vm:\n            for name, param in self.visual_encoder.named_parameters():\n                param.requires_grad = False\n            print(f'Loading Frozen vision encoder: {args.vision_model} -- Done')\n        else:\n            print(f'Loading Trainable vision encoder: {args.vision_model} -- Done')\n\n        print('Loading LLM model')\n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            args.llm_model, \n            use_fast=False,\n            trust_remote_code=True\n        )\n    \n        print(f\"BOS token ID: {self.tokenizer.bos_token_id}\")\n        print(f\"EOS token ID: {self.tokenizer.eos_token_id}\")\n        print(f\"PAD token ID: {self.tokenizer.pad_token_id}\")\n        \n        # Load LLM model\n        if args.low_resource:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                args.llm_model,\n                torch_dtype=torch.bfloat16,\n                load_in_8bit=True,\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n        else:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                args.llm_model,\n                torch_dtype=torch.bfloat16,\n                trust_remote_code=True\n            )\n\n        if args.llm_use_lora:\n            self.embed_tokens = self.model.get_input_embeddings()\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM, \n                inference_mode=False, \n                r=args.llm_r, \n                lora_alpha=args.llm_alpha, \n                lora_dropout=args.lora_dropout,\n                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  \n            )\n            self.model = get_peft_model(self.model, peft_config)\n            self.model.print_trainable_parameters()\n            print('Loading LLM LoRA Done')         \n        else:\n            self.embed_tokens = self.model.get_input_embeddings()\n            for name, param in self.model.named_parameters():\n                param.requires_grad = False\n            print('Loading LLM Done')\n        \n        # --- REPLACED MLP WITH HYBRID VISION ABSTRACTOR ---\n        # NOTE: Renamed self.proj to self.llm_proj to imply complex connector, \n        # but logic matches the Hybrid structure.\n        self.visual_hidden_size = self.visual_encoder.config.hidden_size\n        self.llm_hidden_size = self.model.config.hidden_size\n        \n        self.llm_proj = HybridVisionAbstractor(\n            visual_dim=self.visual_hidden_size,\n            llm_dim=self.llm_hidden_size,\n            num_queries=64\n        )\n        \n        # Initialize concepts\n        self.disease_concepts = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Enlarged Cardiomediastinum\",\n                                 \"Fracture\", \"Lung Lesion\", \"Lung Opacity\", \"No Finding\", \"Pleural Effusion\",\n                                 \"Pleural Other\", \"Pneumonia\", \"Pneumothorax\", \"Support Devices\"]\n        self._inject_diseases_into_connector()\n\n        self.layer_norm = nn.LayerNorm(self.model.config.hidden_size)\n        \n        self.end_sym = \"<|im_end|>\"\n\n        self.system_prompt = \"<|im_start|>system\\nYou are a professional radiologist. Please answer the question based on the chest X-ray image and choose from the following two options: [yes, no].<|im_end|>\\n\"\n\n        self.val_step_outputs = []\n        self.test_step_outputs = []\n        self.val_score = 0.0\n\n    def _inject_diseases_into_connector(self):\n        \"\"\"Helper to inject disease embeddings into the Hybrid Abstractor\"\"\"\n        device = self.embed_tokens.weight.device\n        concept_vectors = []\n        for concept in self.disease_concepts:\n            # Use self.tokenizer as defined in init\n            ids = self.tokenizer(concept, return_tensors=\"pt\", add_special_tokens=False).input_ids\n            # Move ids to correct device (if model is already on GPU)\n            if hasattr(self.embed_tokens.weight, 'device'):\n                ids = ids.to(self.embed_tokens.weight.device)\n                \n            with torch.no_grad():\n                embeds = self.embed_tokens(ids)\n            concept_vec = embeds.mean(dim=1).squeeze(0)\n            concept_vectors.append(concept_vec)\n        \n        if len(concept_vectors) > 0:\n            concept_stack = torch.stack(concept_vectors)\n            self.llm_proj.inject_concept_init(concept_stack)\n        \n    def score(self, ref, hypo):\n        \"\"\"\n        ref, dictionary of reference sentences (id, sentence)\n        hypo, dictionary of hypothesis sentences (id, sentence)\n        score, dictionary of scores\n        \"\"\"\n        scorers = [\n            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n            (Rouge(), \"ROUGE_L\"),\n            (Cider(), \"CIDEr\")\n        ]\n        final_scores = {}\n        for scorer, method in scorers:\n            score, scores = scorer.compute_score(ref, hypo)\n            if type(score) == list:\n                for m, s in zip(method, score):\n                    final_scores[m] = s\n            else:\n                final_scores[method] = score\n        \n        return final_scores\n\n    def encode_img(self, images):\n        \"\"\"\n        Updated encode_img to work with HybridVisionAbstractor.\n        Input images: Tensor (B, 3, H, W) or List[Tensor]\n        \"\"\"\n        # Ensure input is a tensor batch for efficiency\n        if isinstance(images, list):\n            # Stack if it's a list of tensors\n            images = torch.stack(images)\n            \n        images = images.to(self.device)\n        \n        # 1. Vision Encoder\n        visual_outputs = self.visual_encoder(images)\n        image_embeds = visual_outputs.last_hidden_state # (B, Seq, Dim)\n        \n        # 2. Hybrid Projector (replaces self.proj)\n        inputs_llama = self.llm_proj(image_embeds) # (B, 64, Dim)\n        \n        # 3. Create Attention Mask\n        atts_llama = torch.ones(inputs_llama.shape[:2], dtype=torch.long).to(images.device)\n        \n        return inputs_llama, atts_llama\n\n    def prompt_wrap(self, img_embeds, atts_img, questions):\n        \"\"\"\n        Wrap image embeddings with Qwen-style prompt including the question.\n        Format: {system_prompt} <user_start> {question} <image> <user_end> <assistant_start>\n        \"\"\"\n        batch_size = img_embeds.shape[0]\n        \n        # Build prompts for each item in the batch\n        wrapped_embeds_list = []\n        wrapped_atts_list = []\n        \n        for i in range(batch_size):\n            question = questions[i] if questions[i] is not None else \"Describe the following image in detail.\"\n            \n            # Construct full prompt with question\n            full_prompt = f\"{self.system_prompt}<|im_start|>user\\n{question} <image><|im_end|>\\n<|im_start|>assistant\\n\"\n            \n            # Split at image placeholder\n            p_before, p_after = full_prompt.split('<image>')\n            \n            # Tokenize prompt parts\n            p_before_tokens = self.tokenizer(\n                p_before,\n                return_tensors=\"pt\",\n                add_special_tokens=False\n            ).to(img_embeds.device)\n            \n            p_after_tokens = self.tokenizer(\n                p_after,\n                return_tensors=\"pt\",\n                add_special_tokens=False\n            ).to(img_embeds.device)\n            \n            # Get embeddings\n            with torch.no_grad():\n                p_before_embeds = self.embed_tokens(p_before_tokens.input_ids)\n                p_after_embeds = self.embed_tokens(p_after_tokens.input_ids)\n            \n            # Concatenate: [prompt_before] + [image] + [prompt_after]\n            # img_embeds[i:i+1] is (1, 64, Dim)\n            wrapped_embeds = torch.cat([\n                p_before_embeds,\n                img_embeds[i:i+1],\n                p_after_embeds\n            ], dim=1)\n            \n            wrapped_embeds_list.append(wrapped_embeds)\n            \n            # Create attention mask\n            wrapped_atts = torch.ones(\n                wrapped_embeds.shape[1],\n                device=img_embeds.device,\n                dtype=atts_img.dtype\n            )\n            wrapped_atts_list.append(wrapped_atts)\n        \n        # Find max sequence length in the batch\n        max_seq_len = max(embeds.shape[1] for embeds in wrapped_embeds_list)\n        \n        # Pad all sequences to the same length\n        padded_embeds_list = []\n        padded_atts_list = []\n        \n        for embeds, atts in zip(wrapped_embeds_list, wrapped_atts_list):\n            seq_len = embeds.shape[1]\n            if seq_len < max_seq_len:\n                # Pad embeddings with zeros\n                padding_size = max_seq_len - seq_len\n                padding = torch.zeros(\n                    embeds.shape[0], \n                    padding_size, \n                    embeds.shape[2],\n                    dtype=embeds.dtype,\n                    device=embeds.device\n                )\n                embeds = torch.cat([embeds, padding], dim=1)\n                \n                # Pad attention mask with zeros (masked positions)\n                atts_padding = torch.zeros(\n                    padding_size,\n                    dtype=atts.dtype,\n                    device=atts.device\n                )\n                atts = torch.cat([atts, atts_padding], dim=0)\n            \n            padded_embeds_list.append(embeds)\n            padded_atts_list.append(atts)\n        \n        # Stack all items in the batch\n        wrapped_img_embeds = torch.cat(padded_embeds_list, dim=0)\n        wrapped_atts_img = torch.stack(padded_atts_list, dim=0)\n        \n        return wrapped_img_embeds, wrapped_atts_img\n\n    def forward(self, samples):\n        image = samples[\"image\"]\n        questions = samples.get(\"question\", [None] * len(samples[\"text\"]))\n        \n        img_embeds, atts_img = self.encode_img(image)\n        img_embeds = self.layer_norm(img_embeds)\n\n        # Wrap image with prompt (now includes question)\n        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, questions)\n\n        self.tokenizer.padding_side = \"right\"\n        text = [t + self.end_sym for t in samples[\"text\"]]\n\n        # Tokenize target text\n        to_regress_tokens = self.tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.hparams.max_length,\n            add_special_tokens=False\n        ).to(img_embeds.device)\n\n        # Create labels: mask prompt+image tokens with -100, keep text tokens\n        targets = to_regress_tokens.input_ids.masked_fill(\n            to_regress_tokens.input_ids == self.tokenizer.pad_token_id, -100\n        )\n\n        # Create empty targets for prompt+image tokens\n        empty_targets = (\n            torch.ones([atts_img.shape[0], atts_img.shape[1]],\n                       dtype=torch.long).to(img_embeds.device).fill_(-100)\n        )\n        targets = torch.cat([empty_targets, targets], dim=1)\n\n        # Get text embeddings\n        with torch.no_grad():\n            to_regress_embeds = self.embed_tokens(to_regress_tokens.input_ids)\n        \n        # Concatenate all embeddings: [prompt+image] + [text]\n        inputs_embeds = torch.cat([img_embeds, to_regress_embeds], dim=1)\n        attention_mask = torch.cat([atts_img, to_regress_tokens.attention_mask], dim=1)\n\n        # Forward through LLM\n        outputs = self.model(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            return_dict=True,\n            labels=targets,\n            use_cache=False\n        )\n        all_loss = outputs.loss\n\n        return {\"loss\": all_loss}\n\n    def training_step(self, batch, batch_idx):\n        result = self(batch)\n        self.log_dict(result, prog_bar=True)\n        return result\n\n    def save_checkpoint(self, eval_res):\n        current_epoch, global_step = self.trainer.current_epoch, self.trainer.global_step\n        param_grad_dic = {\n            k: v.requires_grad for (k, v) in self.named_parameters() if v.requires_grad\n        }\n        state_dict = self.state_dict()\n        for k in list(state_dict.keys()):\n            if k not in param_grad_dic.keys():\n                del state_dict[k]\n        \n        save_obj = {\n            \"state_dict\": state_dict,\n            \"hyper_parameters\": self.hparams,\n            \"pytorch-lightning_version\": pl.__version__,\n            \"epoch\": current_epoch,\n            \"global_step\": global_step,\n        }\n        os.makedirs(os.path.join(self.hparams.savedmodel_path, 'checkpoints'), exist_ok=True)\n        save_to = os.path.join(\n            self.hparams.savedmodel_path, 'checkpoints',\n            \"checkpoint_epoch{}_step{}_rougle_l{:3}_bleu{:3f}_cider{:3f}.pth\".format(\n                current_epoch, global_step, eval_res['ROUGE_L'],eval_res['Bleu_4'], eval_res['CIDEr']\n            ),\n        )\n        self.print(\"Saving checkpoint at step {} to {}.\".format(global_step, save_to))\n        torch.save(save_obj, save_to)\n    \n    @torch.no_grad()\n    def validation_forward(self, samples):\n        \"\"\"\n        Validation forward:\n        - Only does generation\n        - Does NOT compute training loss\n        \"\"\"\n        self.model.eval()\n    \n        image = samples[\"image\"]\n        questions = samples.get(\"question\", [None] * len(image))\n        refs = samples[\"text\"]  # ground truth answers\n    \n        # ---- Encode image ----\n        img_embeds, atts_img = self.encode_img(image)\n        img_embeds = self.layer_norm(img_embeds)\n    \n        # ---- Build the multimodal prompt ----\n        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, questions)\n    \n        # Cast to LLM dtype (important for BF16 or FP16)\n        dtype = self.model.dtype\n        img_embeds = img_embeds.to(dtype)\n    \n        # ---- Generation ----\n        outputs = self.model.generate(\n            inputs_embeds=img_embeds,\n            attention_mask=atts_img,\n            num_beams=self.hparams.beam_size,\n            do_sample=self.hparams.do_sample,\n            min_new_tokens=self.hparams.min_new_tokens,\n            max_new_tokens=self.hparams.max_new_tokens,\n            repetition_penalty=self.hparams.repetition_penalty,\n            length_penalty=self.hparams.length_penalty,\n            temperature=self.hparams.temperature,\n            pad_token_id=self.tokenizer.pad_token_id,\n            eos_token_id=self.tokenizer.eos_token_id,\n        )\n    \n        # ---- Decode outputs ----\n        preds = [self.decode(o) for o in outputs]\n    \n        return preds, refs, questions\n\n    def validation_step(self, batch, batch_idx):\n        preds, refs, questions = self.validation_forward(batch)\n    \n        # Save for epoch-end scoring\n        self.val_step_outputs.append({\n            \"preds\": preds,\n            \"refs\": refs,\n            \"questions\": questions,\n            \"ids\": batch[\"id\"],\n        })\n    \n        # Optionally log dummy val loss (for progress bar)\n        dummy_loss = torch.tensor(0.0, device=self.device)\n        self.log(\"val_loss\", dummy_loss, prog_bar=True, on_step=False, on_epoch=True)\n    \n        return preds\n\n    def decode(self, output_token):\n        \"\"\"Decode output tokens to text.\"\"\"\n        # Remove special tokens at the beginning\n        if len(output_token) > 0 and output_token[0] == self.tokenizer.pad_token_id:\n            output_token = output_token[1:]\n        if len(output_token) > 0 and output_token[0] == self.tokenizer.bos_token_id:\n            output_token = output_token[1:]\n        \n        # Decode to text\n        output_text = self.tokenizer.decode(output_token, add_special_tokens=False)\n        \n        # Split at end symbol and clean up\n        output_text = output_text.split(self.end_sym)[0].strip()\n        \n        # Remove Qwen special tokens\n        output_text = output_text.replace('<|im_start|>', '')\n        output_text = output_text.replace('<|im_end|>', '')\n        output_text = output_text.replace('<|endoftext|>', '')\n        output_text = output_text.replace('<unk>', '')\n        \n        return output_text\n\n    def on_validation_epoch_end(self):\n        if len(self.val_step_outputs) == 0:\n            return\n    \n        preds, refs, ids = [], [], []\n    \n        for out in self.val_step_outputs:\n            preds.extend(out[\"preds\"])\n            refs.extend(out[\"refs\"])\n            ids.extend(out[\"ids\"])\n    \n        # Build dicts for evalcap scoring\n        ref_dict = {k: [v] for k, v in zip(ids, refs)}\n        pred_dict = {k: [v] for k, v in zip(ids, preds)}\n    \n        metrics = self.score(ref_dict, pred_dict)\n        self.log_dict(metrics, prog_bar=True, logger=True)\n    \n        # Save JSON\n        result_folder = os.path.join(self.hparams.savedmodel_path, 'result')\n        os.makedirs(result_folder, exist_ok=True)\n    \n        json.dump(pred_dict, open(os.path.join(result_folder, f\"val_pred.json\"), 'w'))\n        json.dump(ref_dict, open(os.path.join(result_folder, f\"val_ref.json\"), 'w'))\n    \n        self.val_step_outputs.clear()\n\n    def test_step(self, samples, batch_idx):\n        self.tokenizer.padding_side = \"right\"\n        text = samples[\"text\"]\n        questions = samples.get(\"question\", [None] * len(text))\n\n        to_regress_tokens = self.tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.hparams.max_length,\n            add_special_tokens=False\n        )\n\n        image = samples[\"image\"]\n        img_embeds, atts_img = self.encode_img(image)\n        img_embeds = self.layer_norm(img_embeds)\n        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, questions)\n\n        batch_size = img_embeds.shape[0]\n        inputs_embeds = img_embeds\n        attention_mask = atts_img\n\n        outputs = self.model.generate(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            num_beams=self.hparams.beam_size,\n            do_sample=self.hparams.do_sample,\n            min_new_tokens=self.hparams.min_new_tokens,\n            max_new_tokens=self.hparams.max_new_tokens,\n            repetition_penalty=self.hparams.repetition_penalty,\n            length_penalty=self.hparams.length_penalty,\n            temperature=self.hparams.temperature,\n            pad_token_id=self.tokenizer.pad_token_id,\n            eos_token_id=self.tokenizer.eos_token_id,\n        )\n        hypo = [self.decode(i) for i in outputs]\n        ref = [self.decode(i) for i in to_regress_tokens['input_ids']]\n        self.test_step_outputs.append({\"hypo\": hypo, \"ref\": ref, \"id\": samples[\"id\"], \"question\": questions})\n        return hypo, ref\n\n    def on_test_epoch_end(self):\n        self.test_step_outputs = self.test_step_outputs\n        ref, hypo, ids = [], [], []\n        \n        if isinstance(self.test_step_outputs[0], dict):\n            for out in self.test_step_outputs:\n                ref.extend(out['ref'])\n                hypo.extend(out['hypo'])\n                ids.extend(out['id'])\n\n        ref = {k: [v] for k, v in zip(ids, ref)}\n        hypo = {k: [v] for k, v in zip(ids, hypo)}\n        eval_res = self.score(ref=ref, hypo=hypo)\n\n        result_folder = os.path.join(self.hparams.savedmodel_path, 'result')\n        os.makedirs(result_folder, exist_ok=True)\n        json.dump(hypo, open(os.path.join(result_folder, \"test_result.json\"), 'w'))\n        json.dump(ref, open(os.path.join(result_folder, 'test_refs.json'), 'w'))\n        self.print(f\"Test result of {self.hparams.delta_file}: {eval_res}\")\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer=optimizer, T_max=self.hparams.max_epochs, eta_min=1e-6\n        )\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n    def get_progress_bar_dict(self):\n        items = super().get_progress_bar_dict()\n        items.pop(\"v_num\", None)\n        return items\n\n    def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\n        optimizer.zero_grad()\n\n    @torch.no_grad()\n    def all_gather(self, data):\n        \"\"\"\n        Performs all_gather operation on the provided tensors.\n        *** Warning ***: torch.distributed.all_gather has no gradient.\n        \"\"\"\n        dist.barrier()\n        gather_data = [None for _ in range(torch.distributed.get_world_size())]\n        dist.all_gather_object(gather_data, data)\n        return gather_data\n\n    def _parse_image(self, img):\n        pixel_values = self.vit_feature_extractor(img, return_tensors=\"pt\").pixel_values\n        return pixel_values[0] \n        \n    @torch.no_grad()\n    def inference(self, image_paths, question=None, beam_size=1, do_sample=False, \n                 min_new_tokens=1, max_new_tokens=100, repetition_penalty=1.0, \n                 length_penalty=1.0, temperature=1.0):\n    \n        self.eval()\n        device = next(self.parameters()).device\n    \n        images = []\n        for image_path in image_paths:\n            with Image.open(image_path) as pil:\n                array = np.array(pil, dtype=np.uint8)\n                if array.shape[-1] != 3 or len(array.shape) != 3:\n                    array = np.array(pil.convert(\"RGB\"), dtype=np.uint8)\n    \n                image = self._parse_image(array)   # (3, H, W)\n                image = image.to(device)           \n                images.append(image)\n    \n        # encode_img sẽ stack -> (B, 3, H, W)\n        img_embeds, atts_img = self.encode_img(images)\n        img_embeds = self.layer_norm(img_embeds)\n    \n        # cast dtype cho LLM\n        img_embeds = img_embeds.to(self.model.dtype)\n\n        if question is None:\n            question = \"Describe the following image in detail.\"\n        questions = [question] if isinstance(question, str) else question\n    \n        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, questions)\n    \n        outputs = self.model.generate(\n            inputs_embeds=img_embeds,\n            attention_mask=atts_img,\n            num_beams=beam_size,\n            do_sample=do_sample,\n            min_new_tokens=min_new_tokens,\n            max_new_tokens=max_new_tokens,\n            repetition_penalty=repetition_penalty,\n            length_penalty=length_penalty,\n            temperature=temperature,\n            pad_token_id=self.tokenizer.pad_token_id,\n            eos_token_id=self.tokenizer.eos_token_id,\n        )\n    \n        return [self.decode(i) for i in outputs]\n    \n            \n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:19:49.802141Z","iopub.execute_input":"2025-12-13T16:19:49.802421Z","iopub.status.idle":"2025-12-13T16:19:49.853610Z","shell.execute_reply.started":"2025-12-13T16:19:49.802402Z","shell.execute_reply":"2025-12-13T16:19:49.852785Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import argparse\nfrom lightning.fabric.utilities.data import AttributeDict\ntorch.serialization.add_safe_globals([AttributeDict])\n\nckpt_file=\"lightmedvlm/checkpoints/epoch=3-step=2468.ckpt\"\n\nargs = AttributeDict({\n    \"vision_model\": \"microsoft/swin-base-patch4-window7-224\",\n    \"llm_model\": \"Qwen/Qwen3-0.6B\",\n\n    # các param còn lại giống train-3\n    \"freeze_vm\": False,\n\n    \"llm_use_lora\": True,\n    \"llm_r\": 8,\n    \"llm_alpha\": 16,\n\n    \"vis_use_lora\": False,\n    \"vis_r\": 8,\n    \"vis_alpha\": 16,\n\n    \"lora_dropout\": 0.1,\n    \"low_resource\": False\n})\n\nmodel = LightMedVLMForInfer.load_from_checkpoint(\n    ckpt_file,\n    strict=False,\n    args=args\n)\nmodel = model.to(\"cuda\")\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:19:50.741620Z","iopub.execute_input":"2025-12-13T16:19:50.741893Z","iopub.status.idle":"2025-12-13T16:19:56.996646Z","shell.execute_reply.started":"2025-12-13T16:19:50.741873Z","shell.execute_reply":"2025-12-13T16:19:56.996006Z"}},"outputs":[{"name":"stdout","text":"Loading vision encoder: microsoft/swin-base-patch4-window7-224\nLoading Trainable vision encoder: microsoft/swin-base-patch4-window7-224 -- Done\nLoading LLM model\nBOS token ID: None\nEOS token ID: 151645\nPAD token ID: 151643\ntrainable params: 2,293,760 || all params: 598,343,680 || trainable%: 0.3834\nLoading LLM LoRA Done\n>> INJECTED 14 DISEASE CONCEPTS <<\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"LightMedVLMForInfer(\n  (visual_encoder): SwinModel(\n    (embeddings): SwinEmbeddings(\n      (patch_embeddings): SwinPatchEmbeddings(\n        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n      )\n      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): SwinEncoder(\n      (layers): ModuleList(\n        (0): SwinStage(\n          (blocks): ModuleList(\n            (0): SwinLayer(\n              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=128, out_features=128, bias=True)\n                  (key): Linear(in_features=128, out_features=128, bias=True)\n                  (value): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): Identity()\n              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (1): SwinLayer(\n              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=128, out_features=128, bias=True)\n                  (key): Linear(in_features=128, out_features=128, bias=True)\n                  (value): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.004347826354205608)\n              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=512, out_features=256, bias=False)\n            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (1): SwinStage(\n          (blocks): ModuleList(\n            (0): SwinLayer(\n              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=256, out_features=256, bias=True)\n                  (key): Linear(in_features=256, out_features=256, bias=True)\n                  (value): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.008695652708411217)\n              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=256, out_features=1024, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=1024, out_features=256, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (1): SwinLayer(\n              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=256, out_features=256, bias=True)\n                  (key): Linear(in_features=256, out_features=256, bias=True)\n                  (value): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.013043479062616825)\n              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=256, out_features=1024, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=1024, out_features=256, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (2): SwinStage(\n          (blocks): ModuleList(\n            (0): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.017391305416822433)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (1): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.021739132702350616)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (2): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.02608695812523365)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (3): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.030434783548116684)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (4): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.03478261083364487)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (5): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.03913043811917305)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (6): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.04347826540470123)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (7): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.04782608896493912)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (8): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.052173912525177)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (9): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.056521736085414886)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (10): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.06086956337094307)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (11): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.06521739065647125)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (12): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.06956521421670914)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (13): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.07391304522752762)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (14): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.0782608687877655)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (15): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.08260869979858398)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (16): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.08695652335882187)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (17): SwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.09130434691905975)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (3): SwinStage(\n          (blocks): ModuleList(\n            (0): SwinLayer(\n              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.09565217792987823)\n              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (1): SwinLayer(\n              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.10000000149011612)\n              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n    )\n    (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (pooler): AdaptiveAvgPool1d(output_size=1)\n  )\n  (model): PeftModelForCausalLM(\n    (base_model): LoraModel(\n      (model): Qwen3ForCausalLM(\n        (model): Qwen3Model(\n          (embed_tokens): Embedding(151936, 1024)\n          (layers): ModuleList(\n            (0-27): 28 x Qwen3DecoderLayer(\n              (self_attn): Qwen3Attention(\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2048, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear(\n                  (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=2048, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n                (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              )\n              (mlp): Qwen3MLP(\n                (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n                (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n                (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n                (act_fn): SiLUActivation()\n              )\n              (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n              (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            )\n          )\n          (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n          (rotary_emb): Qwen3RotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n      )\n    )\n  )\n  (embed_tokens): Embedding(151936, 1024)\n  (llm_proj): HybridVisionAbstractor(\n    (conv_block): Sequential(\n      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU()\n      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): SiLU()\n    )\n    (vis_proj): Linear(in_features=1024, out_features=1024, bias=True)\n    (cross_attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n    )\n    (ln_q): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (ln_v): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (ln_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (mlp): Sequential(\n      (0): Linear(in_features=1024, out_features=4096, bias=True)\n      (1): GELU(approximate='none')\n      (2): Linear(in_features=4096, out_features=1024, bias=True)\n    )\n  )\n  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Infer IU","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nTEST_ANNOT = \"/kaggle/input/mimic-700-images/test/test/vqa/iuxray_test.jsonl\"\nTEST_IMG_DIR = \"/kaggle/input/mimic-700-images/iu_images/iu_images\"\n\nwith open(TEST_ANNOT, \"r\") as f:\n    lines = f.readlines()\n\nprint(\"Số lượng test samples:\", len(lines))\n\nresults = []\n\nmodel.eval()\n\nwith torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n    for line in tqdm(lines):\n        item = json.loads(line)\n\n        img_path = os.path.join(TEST_IMG_DIR, item[\"image\"])\n\n        raw_question = item[\"question\"]\n        question = (\n            raw_question\n            .replace(\"\\n<image>\", \"\")\n            .replace(\"<image>\", \"\")\n            .strip()\n        )\n\n        gt_answer = item[\"answer\"]\n\n        pred = model.inference(\n            image_paths=[img_path],\n            question=question\n        )[0]\n\n        results.append({\n            \"id\": item[\"question_id\"],\n            \"image\": item[\"image\"],\n            \"question\": question,\n            \"gt_answer\": gt_answer,\n            \"pred_answer\": pred\n        })\n\nOUT_FILE = \"iuxray_vqa_advance_predictions.json\"\n\nwith open(OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n    json.dump(results, f, indent=4, ensure_ascii=False)\n\nprint(f\"Saved {len(results)} predictions to {OUT_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:20:04.977200Z","iopub.execute_input":"2025-12-13T16:20:04.977901Z","iopub.status.idle":"2025-12-13T17:13:02.401196Z","shell.execute_reply.started":"2025-12-13T16:20:04.977879Z","shell.execute_reply":"2025-12-13T17:13:02.400448Z"}},"outputs":[{"name":"stdout","text":"Số lượng test samples: 2573\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/2573 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n100%|██████████| 2573/2573 [52:57<00:00,  1.23s/it] ","output_type":"stream"},{"name":"stdout","text":"Saved 2573 predictions to iuxray_vqa_advance_predictions.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Infer MIMIC","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nTEST_ANNOT = \"/kaggle/input/mimic-700-images/test/test/vqa/mimic_test.jsonl\"\nTEST_IMG_DIR = \"/kaggle/input/mimic-700-images/mimic_cxr_selected_224/mimic_cxr_selected_224\"\n\nwith open(TEST_ANNOT, \"r\") as f:\n    lines = f.readlines()\n\nprint(\"Số lượng test samples:\", len(lines))\n\nresults = []\n\nmodel.eval()\n\nwith torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n    for line in tqdm(lines):\n        item = json.loads(line)\n\n        img_path = os.path.join(TEST_IMG_DIR, item[\"image\"])\n\n        raw_question = item[\"question\"]\n        question = (\n            raw_question\n            .replace(\"\\n<image>\", \"\")\n            .replace(\"<image>\", \"\")\n            .strip()\n        )\n\n        gt_answer = item[\"answer\"]\n\n        pred = model.inference(\n            image_paths=[img_path],\n            question=question\n        )[0]\n\n        results.append({\n            \"id\": item[\"question_id\"],\n            \"image\": item[\"image\"],\n            \"question\": question,\n            \"gt_answer\": gt_answer,\n            \"pred_answer\": pred\n        })\n\nOUT_FILE = \"mimic_vqa_advance_predictions.json\"\n\nwith open(OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n    json.dump(results, f, indent=4, ensure_ascii=False)\n\nprint(f\"Saved {len(results)} predictions to {OUT_FILE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T17:14:03.092450Z","iopub.execute_input":"2025-12-13T17:14:03.093015Z","iopub.status.idle":"2025-12-13T18:24:45.676793Z","shell.execute_reply.started":"2025-12-13T17:14:03.092993Z","shell.execute_reply":"2025-12-13T18:24:45.676001Z"}},"outputs":[{"name":"stdout","text":"Số lượng test samples: 3470\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3470/3470 [1:10:42<00:00,  1.22s/it]","output_type":"stream"},{"name":"stdout","text":"Saved 3470 predictions to iuxray_vqa_advance_predictions.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14}]}